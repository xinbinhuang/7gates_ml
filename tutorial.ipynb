{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Components\n",
    "\n",
    "- `Webserver`\n",
    "- `Scheduler`\n",
    "- `Workers`\n",
    "\n",
    "## Kep Concepts:\n",
    "\n",
    "| Concept | What is that | \n",
    "| ---     | --- |\n",
    "| `DAG` - Directed Acyclic Graph | <ul><li>a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.</li><li>a description of the order in which work should take place</li></ul> |\n",
    "| `DAG Definition file` (python script) |  <ul><li> a configuration file specicying the **DAG's structure** as code </li></ul> |\n",
    "| `Operators` (a *Class* object):  | <ul><li>a class that acts as a template for carrying out some work</li><li>usually *atomic*</li><li>run independently - i.e. different machines</li></ul> |\n",
    "|  `Tasks` | <ul><li>a parameterized instance of an operator</li><li>a node in a DAG</li></ul> | \n",
    "|  `Task Instances` | <ul><li>a specific run of a task - i.e. combination of *a dag, a task, and a point in time*</li><li>Have states: \"running\", 'success\", \"failed\", \"skipped\", \"up for retry\" etc.</li></ul> | \n",
    "    \n",
    "Airflow will execute the code in each file to dynamically build the DAG objects.  \n",
    "You can have as many DAGs as you want, each describing an arbitrary number of tasks.   \n",
    "In general, each one should correspond to a single logical workflow.\n",
    "\n",
    "Notes:\n",
    "    - Tasks defined in the DAG will run in a different context - i.e. the workers\n",
    "    - DAG definition file is just a configureation file (i.e. no data processing, no data processing, no data ....)\n",
    "    - Dag is evaluated quickly by the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precedence rules for a task are as follows:\n",
    "\n",
    "- Explicitly passed arguments\n",
    "- Values that exist in the default_args dictionary\n",
    "- The operator’s default value, if one exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templating\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7) }}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "- The `params` hook in `BaseOperator` allows you to pass a dictionary of parameters and/or objects to your templates. \n",
    "- Files can also be passed to the bash_command argument, like bash_command='templated_command.sh' \n",
    "    - where the file location is relative to the directory containing the pipeline file (`tutorial.py` in this case). \n",
    "        - i.e. separating your script’s logic and pipeline code, allowing for proper code highlighting in files composed in different languages, \n",
    "        - general flexibility in structuring pipelines. \n",
    "        - also possible to define your `template_searchpath` as pointing to *any folder locations* in the DAG constructor call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More stuff: \n",
    "- [Jinja documentation](http://jinja.pocoo.org/docs/dev/api/#writing-filters)\n",
    "- Variables and Macros: [Macros reference](https://airflow.apache.org/macros.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting dependencies\n",
    "\n",
    "```python\n",
    "t1.set_downstream(t2)\n",
    "\n",
    "# This means that t2 will depend on t1\n",
    "# running successfully to run.\n",
    "# It is equivalent to:\n",
    "t2.set_upstream(t1)\n",
    "\n",
    "# The bit shift operator can also be\n",
    "# used to chain operations:\n",
    "t1 >> t2\n",
    "\n",
    "# And the upstream dependency with the\n",
    "# bit shift operator:\n",
    "t2 << t1\n",
    "\n",
    "# Chaining multiple dependencies becomes\n",
    "# concise with the bit shift operator:\n",
    "t1 >> t2 >> t3\n",
    "\n",
    "# A list of tasks can also be set as\n",
    "# dependencies. These operations\n",
    "# all have the same effect:\n",
    "t1.set_downstream([t2, t3])\n",
    "t1 >> [t2, t3]\n",
    "[t2, t3] << t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "1. **Running the Script**\n",
    "\n",
    "```python\n",
    "# put your file in the `dags` folder as specified in the `airflow.cfg`\n",
    "python ~/path/to/dags-folder/script.py\n",
    "```\n",
    "2. **Command Line Metadata Validation**\n",
    "\n",
    "```python\n",
    "# print the list of active DAGs\n",
    "airflow list_dags\n",
    "\n",
    "# prints the list of tasks the \"tutorial\" dag_id\n",
    "airflow list_tasks tutorial\n",
    "\n",
    "# prints the hierarchy of tasks in the tutorial DAG\n",
    "airflow list_tasks tutorial --tree\n",
    "```\n",
    "3. **Testing**  \n",
    "Let’s test by running the actual task instances on a specific date.   \n",
    "The date specified in this context is an `execution_date`, which simulates the scheduler running your task or dag at a specific date + time:\n",
    "\n",
    "```python\n",
    "# command layout: command subcommand dag_id task_id date\n",
    "\n",
    "# testing print_date\n",
    "airflow test tutorial print_date 2015-06-01\n",
    "\n",
    "# testing sleep\n",
    "airflow test tutorial sleep 2015-06-01\n",
    "\n",
    "# testing templated\n",
    "airflow test tutorial templated 2015-06-01\n",
    "```\n",
    "> *Note*: airflow `test` command is only for testing  \n",
    "> - run task instances locally, \n",
    "> - outputs their log to stdout (on screen), \n",
    "> - doesn’t bother with dependencies, \n",
    "> - doesn’t communicate state (running, success, failed, …) to the database.\n",
    "\n",
    "\n",
    "4. **Backfill**\n",
    "    - respect your dependencies\n",
    "    - emit logs into files \n",
    "    - talk to the database to record status.\n",
    "    - track progress on webserver \n",
    "    \n",
    "    ? `depends_on_past`?\n",
    "    \n",
    "    \n",
    "```python\n",
    "# optional, start a web server in debug mode in the background\n",
    "# airflow webserver --debug &\n",
    "\n",
    "# start your backfill on a date range\n",
    "airflow backfill tutorial -s 2015-06-01 -e 2015-06-07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next\n",
    "\n",
    "Check these sections\n",
    "- UI \n",
    "- Command line interface\n",
    "- Operators\n",
    "- Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
