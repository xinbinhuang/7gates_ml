{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airflow Tutorial\n",
    "\n",
    "> Airflow is a platform to programmatically author, schedule and monitor workflows.\n",
    "> Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks.\n",
    "\n",
    "### Principles\n",
    "- **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.\n",
    "- **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.\n",
    "- **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.\n",
    "- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers. Airflow is ready to scale to infinity.\n",
    "- ** this is not for streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A exmaple DAG\n",
    "- [Example dag from the Airflow](./dags/tutorial.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture Components\n",
    "\n",
    "- `Webserver`\n",
    "- `Scheduler`\n",
    "- `Workers`\n",
    "\n",
    "## Kep Concepts:\n",
    "\n",
    "| Concept | What is that | \n",
    "| ---     | --- |\n",
    "| `DAG` - Directed Acyclic Graph | <ul><li>a collection of all the tasks you want to run, organized in a way that reflects their relationships and dependencies.</li><li>a description of the order in which work should take place</li></ul> |\n",
    "| `DAG Definition file` (python script) |  <ul><li> a configuration file specicying the **DAG's structure** as code </li></ul> |\n",
    "| `Operators` (a *Class* object):  | <ul><li>a class that acts as a template for carrying out some work</li><li>usually *atomic*</li><li>run independently - i.e. different machines</li></ul> |\n",
    "|  `Tasks` | <ul><li>a parameterized instance of an operator</li><li>a node in a DAG</li></ul> | \n",
    "|  `Task Instances` | <ul><li>a specific run of a task - i.e. combination of *a dag, a task, and a point in time*</li><li>Have states: \"running\", 'success\", \"failed\", \"skipped\", \"up for retry\" etc.</li></ul> | \n",
    "    \n",
    "Airflow will execute the code in each file to dynamically build the DAG objects.  \n",
    "You can have as many DAGs as you want, each describing an arbitrary number of tasks.   \n",
    "In general, each one should correspond to a single logical workflow.\n",
    "\n",
    "Notes:\n",
    "    - Tasks defined in the DAG will run in a different context - i.e. the workers\n",
    "    - DAG definition file is just a configureation file (i.e. no data processing, no data processing, no data ....)\n",
    "    - Dag is evaluated quickly by the scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Arguments & Tasks\n",
    "\n",
    "- Default arguments are arguments that you pass into a new DAG object which will cascade down to its operators if not otherwise overwritten\n",
    "- Attach tasks to a dag\n",
    "- `task_id` is unique identifier for each task\n",
    "\n",
    "```python\n",
    "# default args\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2015, 6, 1),\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    # 'queue': 'bash_queue',\n",
    "    # 'pool': 'backfill',\n",
    "    # 'priority_weight': 10,\n",
    "    # 'end_date': datetime(2016, 1, 1),\n",
    "}\n",
    "\n",
    "# new dags \n",
    "dag = DAG(\n",
    "    'tutorial', default_args=default_args, schedule_interval=timedelta(days=1))\n",
    "\n",
    "# tasks\n",
    "t1 = BashOperator(\n",
    "    task_id='print_date',\n",
    "    bash_command='date',\n",
    "    dag=dag)\n",
    "\n",
    "t2 = BashOperator(\n",
    "    task_id='sleep',\n",
    "    bash_command='sleep 5',\n",
    "    retries=3,\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "The precedence rules of arguments for a task are as follows:\n",
    "\n",
    "- Explicitly passed arguments\n",
    "- Values that exist in the default_args dictionary\n",
    "- The operator’s default value, if one exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Templating\n",
    "\n",
    "```python\n",
    "templated_command = \"\"\"\n",
    "    {% for i in range(5) %}\n",
    "        echo \"{{ ds }}\"\n",
    "        echo \"{{ macros.ds_add(ds, 7) }}\"\n",
    "        echo \"{{ params.my_param }}\"\n",
    "    {% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t3 = BashOperator(\n",
    "    task_id='templated',\n",
    "    bash_command=templated_command,\n",
    "    params={'my_param': 'Parameter I passed in'},\n",
    "    dag=dag)\n",
    "```\n",
    "\n",
    "- The `params` hook in `BaseOperator` allows you to pass a dictionary of parameters and/or objects to your templates. \n",
    "- Files can also be passed to the bash_command argument, like bash_command='templated_command.sh' \n",
    "    - where the file location is relative to the directory containing the pipeline file (`tutorial.py` in this case). \n",
    "        - i.e. separating your script’s logic and pipeline code, allowing for proper code highlighting in files composed in different languages, \n",
    "        - general flexibility in structuring pipelines. \n",
    "        - also possible to define your `template_searchpath` as pointing to *any folder locations* in the DAG constructor call.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More stuff: \n",
    "- [Jinja documentation](http://jinja.pocoo.org/docs/dev/api/#writing-filters)\n",
    "- Variables and Macros: [Macros reference](https://airflow.apache.org/macros.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting dependencies\n",
    "\n",
    "```python\n",
    "t1.set_downstream(t2)\n",
    "\n",
    "# This means that t2 will depend on t1\n",
    "# running successfully to run.\n",
    "# It is equivalent to:\n",
    "t2.set_upstream(t1)\n",
    "\n",
    "# The bit shift operator can also be\n",
    "# used to chain operations:\n",
    "t1 >> t2\n",
    "\n",
    "# And the upstream dependency with the\n",
    "# bit shift operator:\n",
    "t2 << t1\n",
    "\n",
    "# Chaining multiple dependencies becomes\n",
    "# concise with the bit shift operator:\n",
    "t1 >> t2 >> t3\n",
    "\n",
    "# A list of tasks can also be set as\n",
    "# dependencies. These operations\n",
    "# all have the same effect:\n",
    "t1.set_downstream([t2, t3])\n",
    "t1 >> [t2, t3]\n",
    "[t2, t3] << t1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "1. **Running the Script**\n",
    "\n",
    "```python\n",
    "# put your file in the `dags` folder as specified in the `airflow.cfg`\n",
    "python ~/path/to/dags-folder/script.py\n",
    "```\n",
    "2. **Command Line Metadata Validation**\n",
    "\n",
    "```python\n",
    "# print the list of active DAGs\n",
    "airflow list_dags\n",
    "\n",
    "# prints the list of tasks the \"tutorial\" dag_id\n",
    "airflow list_tasks tutorial\n",
    "\n",
    "# prints the hierarchy of tasks in the tutorial DAG\n",
    "airflow list_tasks tutorial --tree\n",
    "```\n",
    "3. **Testing**  \n",
    "Let’s test by running the actual task instances on a specific date.   \n",
    "The date specified in this context is an `execution_date`, which simulates the scheduler running your task or dag at a specific date + time:\n",
    "\n",
    "```python\n",
    "# command layout: command subcommand dag_id task_id date\n",
    "\n",
    "# testing print_date\n",
    "airflow test tutorial print_date 2015-06-01\n",
    "\n",
    "# testing sleep\n",
    "airflow test tutorial sleep 2015-06-01\n",
    "\n",
    "# testing templated\n",
    "airflow test tutorial templated 2015-06-01\n",
    "```\n",
    "> *Note*: airflow `test` command is only for testing  \n",
    "> - run task instances locally, \n",
    "> - outputs their log to stdout (on screen), \n",
    "> - doesn’t bother with dependencies, \n",
    "> - doesn’t communicate state (running, success, failed, …) to the database.\n",
    "\n",
    "\n",
    "4. **Backfill**\n",
    "    - respect your dependencies\n",
    "    - emit logs into files \n",
    "    - talk to the database to record status.\n",
    "    - track progress on webserver \n",
    "    \n",
    "    \n",
    "    \n",
    "```python\n",
    "# optional, start a web server in debug mode in the background\n",
    "# airflow webserver --debug &\n",
    "\n",
    "# start your backfill on a date range\n",
    "airflow backfill tutorial -s 2015-06-01 -e 2015-06-07\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other examples\n",
    "\n",
    "- dynamic pipeline generation DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other reads: \n",
    "\n",
    "Check these sections in the [Airflow Doc](https://airflow.apache.org/index.html)\n",
    "- UI \n",
    "- Command line interface\n",
    "- Operators\n",
    "- Macros\n",
    "\n",
    "#### Plugins\n",
    "\n",
    "Airlfow out-of-the-box has some common operators to work with different data sources: .e.g Hive, S3, MySql, Redshift.. You can also build your own if you want.\n",
    "\n",
    "- [A github organization repo for Airflow Plugins](https://github.com/airflow-plugins)\n",
    "\n",
    "#### Dynamic Pipeline Generation\n",
    "\n",
    "- One DAG file that read config files and generate >200 DAGs [Wepay](https://wecode.wepay.com/posts/airflow-wepay)\n",
    "- [Airbnb experimentaion platform](https://medium.com/airbnb-engineering/https-medium-com-jonathan-parks-scaling-erf-23fd17c91166)\n",
    "- [Examples by Astronomer.io](https://www.astronomer.io/guides/dynamically-generating-dags/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
